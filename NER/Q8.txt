4.Evaluation Metric

I think CONLL is better. Take one result as an example.

### evaluation of data/twitter_dev.ner; writing to ./twitter_dev.ner.pred
Token-wise accuracy 96.1601802895
Token-wise F1 (macro) 31.1923370622
Token-wise F1 (micro) 96.1601802895
Sentence-wise accuracy 68.813559322
               precision    recall  f1-score   support

    B-company       0.81      0.36      0.50        36
   B-facility       0.62      0.36      0.45        28
    B-geo-loc       0.69      0.32      0.44        77
      B-movie       0.00      0.00      0.00         7
B-musicartist       0.50      0.08      0.13        13
      B-other       0.67      0.19      0.30        63
     B-person       0.73      0.60      0.66       108
    B-product       0.75      0.16      0.26        19
 B-sportsteam       0.50      0.18      0.27        11
     B-tvshow       0.25      0.09      0.13        11
    I-company       0.00      0.00      0.00         7
   I-facility       0.81      0.45      0.58        29
    I-geo-loc       0.50      0.07      0.12        14
      I-movie       0.00      0.00      0.00        11
I-musicartist       0.50      0.07      0.12        15
      I-other       0.70      0.20      0.31        81
     I-person       0.82      0.67      0.74        61
    I-product       1.00      0.25      0.40        16
 I-sportsteam       0.00      0.00      0.00         4
     I-tvshow       0.33      0.10      0.15        10
            O       0.97      1.00      0.98     10916

  avg / total       0.95      0.96      0.95     11537

processed 11537 tokens with 373 phrases; found: 191 phrases; correct: 122.
accuracy:  96.16%; precision:  63.87%; recall:  32.71%; FB1:  43.26
          company: precision:  81.25%; recall:  36.11%; FB1:  50.00  16
         facility: precision:  62.50%; recall:  35.71%; FB1:  45.45  16
          geo-loc: precision:  66.67%; recall:  31.17%; FB1:  42.48  36
            movie: precision:   0.00%; recall:   0.00%; FB1:   0.00  1
      musicartist: precision:   0.00%; recall:   0.00%; FB1:   0.00  2
            other: precision:  50.00%; recall:  14.29%; FB1:  22.22  18
           person: precision:  66.67%; recall:  55.56%; FB1:  60.61  90
          product: precision:  75.00%; recall:  15.79%; FB1:  26.09  4
       sportsteam: precision:  50.00%; recall:  18.18%; FB1:  26.67  4
           tvshow: precision:  25.00%; recall:   9.09%; FB1:  13.33  4

The tables of specific types in two metrics are similar, which include precision, recall and FB1. The support metric in provided python script may not matter too much for developers. The summary data in CONLL seems better and clearer. It provides total numbers of tokens and phrases, also correct numbers out of found ones. While Sentence-wise accuracy in provided python script may support same thing, but is not as clear as that in CONLL.

